# HPA for deployment

## Описание.
Создадим приложение и автоматически поскейлим его при подаче нагрузки.

## Поехали!
```
apiVersion: v1
kind: Namespace
metadata:
  name: autoscale-test
  labels:
    env: test
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cam-deployment-hpa
  namespace: autoscale-test
  labels:
    cam: deploy-for-hpa
    env: test
  annotations:
    author: cameda
spec:
  replicas: 1
  selector:
    matchLabels:
      app: autoscale
  template:
    metadata:
      name: pod-autoscale
      labels:
        app: autoscale
    spec:
      containers:
        - name: container-autoscale
          image: k8s.gcr.io/hpa-example
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              memory: "256Mi"
              cpu: "0.5"
            limits:
              memory: "500Mi"
              cpu: "1"
          ports:
          - containerPort: 80
          - containerPort: 443
          livenessProbe:
            failureThreshold: 10
            successThreshold: 1
            httpGet:
              path: /
              port: 80
            periodSeconds: 10
            timeoutSeconds: 1
            initialDelaySeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: cam-service-hpa
  namespace: autoscale-test
  labels:
    env: test
    cam: srv-for-hpa
  annotations:
    author: cameda
spec:
  selector:
    app: autoscale
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer
```

### Создаём HPA. Смотрит за подами деплоймента cam-deployment-hpa.
```
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: cam-hpa
  namespace: autoscale-test
  labels:
    env: test
    cam: hpa
  annotations:
    author: cameda
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cam-deployment-hpa
  minReplicas: 1
  maxReplicas: 7
  targetCPUUtilizationPercentage: 20
```

### Тестируем работу HPA.
```
LOAD_BALANCER_IP=<IP-адрес балансировщика>
while true; do wget -q -O- http://$LOAD_BALANCER_IP; done > /dev/null
```

### Пример.
```
### После создания деплоймента.
kubectl get po -n autoscale-test
NAME                                 READY   STATUS    RESTARTS   AGE
cam-deployment-hpa-996566d8b-vqmjx   1/1     Running   0          2m38s
```

```
### После подачи нагрузки.
kubectl get po -n autoscale-test
NAME                                 READY   STATUS    RESTARTS   AGE
cam-deployment-hpa-996566d8b-5hw4x   1/1     Running   0          3m31s
cam-deployment-hpa-996566d8b-c9775   0/1     Pending   0          3m1s
cam-deployment-hpa-996566d8b-lkch5   0/1     Pending   0          91s
cam-deployment-hpa-996566d8b-rkmrv   1/1     Running   0          3m31s
cam-deployment-hpa-996566d8b-sfnws   0/1     Pending   0          91s
cam-deployment-hpa-996566d8b-v8xdr   0/1     Pending   0          3m1s
cam-deployment-hpa-996566d8b-vqmjx   1/1     Running   0          8m42s

kubectl get hpa -n autoscale-test
NAME      REFERENCE                       TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
cam-hpa   Deployment/cam-deployment-hpa   38%/20%   1         7         7          7m59s
```

```
### Снимаем нагрузку.
kubectl get po -n autoscale-test
NAME                                 READY   STATUS    RESTARTS   AGE
cam-deployment-hpa-996566d8b-rkmrv   1/1     Running   0          35m
```

### Результат.
После создания Dedployment был создан один под, как и задумывалось. После того как подали нагрузку, количество подов было увеличено, чтобы эту нагрузку держать. А когда нагрузка спала, то HPA отскейлил Deployment обратно до одного пода.
